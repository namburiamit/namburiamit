# Amit Namburi

**Graduate Student, Computer Science, University of California San Diego**  
Advisor: Julian McAuley  
Research Focus: Multimodal AI, Conversational Recommender Systems, Audio Ã— ML  

ðŸ“§ [anamburi@ucsd.edu](mailto:anamburi@ucsd.edu)  
ðŸ”— [LinkedIn](https://linkedin.com/in/amit-namburi) | [GitHub](https://github.com/namburiamit) | [Google Scholar](https://scholar.google.com/citations?user=vPtFpgIAAAAJ) | [Website](https://namburiamit.com)

---

## About

I am a graduate student in Computer Science at UC San Diego, working at the intersection of **multimodal AI**, **LLMs**, and **music intelligence**.  
My work focuses on building **conversational recommender systems**, **audio-language alignment models**, and **scalable multimodal frameworks** that connect sound, language, and reasoning.  

At the **McAuley Lab**, I design large-scale models that learn from audio, text, and visual data for retrieval, recommendation, and generation.  
My current research involves **efficient fine-tuning (LoRA/QLoRA)**, **reinforcement-based data selection**, and **temporal modeling** for multimodal understanding and conversational intelligence.

---

## Education

**University of California, San Diego**  
- **M.S. in Computer Science (Machine Learning)**, 2025â€“2026, GPA: 4.0  
  Graduate TA: CSE 153/253 (ML for Music), CSE 158/258 (Recommender Systems and Web Mining)  
- **B.S. in Computer Science**, 2021â€“2025, GPA: 3.7, Provost Honors

---

## Experience

- **Software Engineer Intern, Apple (Core OS)**  
  Built data systems and LLM-based audio evaluation frameworks for scalable performance monitoring.  
- **Graduate Research Assistant, McAuley Lab (UCSD)**  
  Conducting research on multimodal learning, conversational recommendation, and model adaptation.  
- **Data Engineer, FDI Lab (UCSD)**  
  Developed scalable ETL and retrieval-augmented systems for large-scale data analysis.

---

## Publications

- **MusiCRS: Benchmarking Audio-Centric Conversational Recommendation**  
  *arXiv preprint, 2025*  
  Benchmark for conversational recommendation grounded in audio context.  
  [arXiv](https://arxiv.org/abs/2509.19469)

- **WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning**  
  *EMNLP 2025*  
  Evaluates multimodal LLMs for symbolic music reasoning in real-world settings.  
  [arXiv](https://arxiv.org/abs/2509.04744)

- **FUTGA-MIR: Enhancing Fine-grained and Temporally-aware Music Understanding with MIR**  
  *ICASSP 2025*  
  Improves fine-grained music retrieval through temporal and generative augmentation.  
  [IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/10888485)

- **CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation**  
  *ICASSP 2025*  
  Proposes contrastive strategies for long-form audioâ€“text pretraining using temporal cues.  
  [IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/10888307)

---

## Research Interests

- Multimodal Learning (Audio, Vision, Language)  
- Conversational Recommender Systems  
- Audio Ã— Machine Learning  
- LLM Adaptation and Fine-tuning  
- Generative Evaluation and Temporal Modeling

---

## Technical Skills

**Languages:** Python, C/C++, Java, TypeScript, SQL  
**ML/AI:** PyTorch, TensorFlow, Hugging Face, LangChain, LlamaIndex, LoRA/QLoRA  
**Systems and Data:** Spark, Docker, AWS, CUDA, REST APIs  
**Web and Infra:** FastAPI, React, Streamlit, Node.js, Kubernetes

---
